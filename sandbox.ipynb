{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append('..')\n",
    "import tensorflow as tf\n",
    "#from supplementary_code_direct_ranker.DirectRanker import directRanker\n",
    "#from supplementary_code_direct_ranker.helpers import readData, nDCGScorer_cls, MAP_cls\n",
    "import DirectRanker as drr\n",
    "import helpers as hlps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_test, y_test, q_test = hlps.readData(data_path=\"/Users/wery/Desktop/BAJiawenWang/dataset/MQ2007/Fold1/test.txt\", \n",
    "#                                        binary=True, at=10, number_features=46, bin_cutoff=1.5, cut_zeros=True)\n",
    "# x_train, y_train, q_train = hlps.readData(data_path=\"/Users/wery/Desktop/BAJiawenWang/dataset/MQ2007/Fold1/train.txt\", \n",
    "#                                           binary=True, at=10, number_features=46, bin_cutoff=1.5, cut_zeros=True)\n",
    "\n",
    "# x_test, y_test, q_test = hlps.readData(data_path=\"/Users/wery/Desktop/BAJiawenWang/dataset/MQ2008/Fold1/test.txt\", \n",
    "#                                        binary=True, at=10, number_features=46, bin_cutoff=1.5, cut_zeros=True)\n",
    "# x_train, y_train, q_train = hlps.readData(data_path=\"/Users/wery/Desktop/BAJiawenWang/dataset/MQ2008/Fold1/train.txt\", \n",
    "#                                           binary=True, at=10, number_features=46, bin_cutoff=1.5, cut_zeros=True)\n",
    "\n",
    "# x_test, y_test, q_test = hlps.readData(data_path=\"/Users/wery/Desktop/BAJiawenWang/dataset/OHSUMED/Feature-min/Fold1/testset.txt\", \n",
    "#                                        binary=True, at=10, number_features=45, bin_cutoff=1.5, cut_zeros=True)\n",
    "# x_train, y_train, q_train = hlps.readData(data_path=\"/Users/wery/Desktop/BAJiawenWang/dataset/OHSUMED/Feature-min/Fold1/trainingset.txt\", \n",
    "#                                           binary=True, at=10, number_features=45, bin_cutoff=1.5, cut_zeros=True)\n",
    "\n",
    "# x_test, y_test, q_test = hlps.readData(data_path=\"/Users/wery/Desktop/BAJiawenWang/dataset/TREC/TD2003/Fold1/testset.txt\", \n",
    "#                                        binary=False, at=10, number_features=44, bin_cutoff=1.5, cut_zeros=True)\n",
    "# x_train, y_train, q_train = hlps.readData(data_path=\"/Users/wery/Desktop/BAJiawenWang/dataset/TREC/TD2003/Fold1/trainingset.txt\", \n",
    "#                                           binary=False, at=10, number_features=44, bin_cutoff=1.5, cut_zeros=True)\n",
    "\n",
    "x_test, y_test, q_test = hlps.readData(data_path=\"/Users/wery/Desktop/BAJiawenWang/dataset/TREC/TD2004/Fold1/testset.txt\", \n",
    "                                       binary=False, at=5, number_features=44, bin_cutoff=1.5, cut_zeros=True)\n",
    "x_train, y_train, q_train = hlps.readData(data_path=\"/Users/wery/Desktop/BAJiawenWang/dataset/TREC/TD2004/Fold1/trainingset.txt\", \n",
    "                                          binary=False, at=5, number_features=44, bin_cutoff=1.5, cut_zeros=True)\n",
    "\n",
    "# x_test, y_test, q_test = hlps.readData(data_path=\"/Users/wery/Desktop/MSLR-WEB10K/Fold1/test.txt\", \n",
    "#                                        binary=True, at=10, number_features=136, bin_cutoff=1.5, cut_zeros=True)\n",
    "# x_train, y_train, q_train = hlps.readData(data_path=\"/Users/wery/Desktop/MSLR-WEB10K/Fold1/train.txt\", \n",
    "#                                           binary=True, at=10, number_features=136, bin_cutoff=1.5, cut_zeros=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Ranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_cost(nn, y0):\n",
    "    return tf.reduce_mean(tf.log(1+tf.exp(nn))-nn)\n",
    "\n",
    "\n",
    "# Load directRanker, train, and test\n",
    "dr = drr.directRanker(\n",
    "    feature_activation=tf.nn.tanh,\n",
    "    ranking_activation=tf.nn.tanh,\n",
    "    # max_steps=10000,\n",
    "    # For debugging\n",
    "    #cost=lambda_cost,\n",
    "    max_steps=10000,\n",
    "    print_step=500,\n",
    "    start_batch_size=3,\n",
    "    end_batch_size=5,\n",
    "    start_qids=20,\n",
    "    end_qids=100,\n",
    "    feature_bias=True,\n",
    "    hidden_layers=[100, 50, 5]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, value: 1.7983719110488892, samples: 3, queries: 20\n",
      "step: 500, value: 0.061399295926094055, samples: 3, queries: 21\n",
      "step: 1000, value: 0.0206648837774992, samples: 3, queries: 23\n",
      "step: 1500, value: 0.05242699012160301, samples: 3, queries: 25\n",
      "step: 2000, value: 0.027360383421182632, samples: 3, queries: 27\n",
      "step: 2500, value: 0.01284741796553135, samples: 3, queries: 29\n",
      "step: 3000, value: 0.05091775953769684, samples: 3, queries: 32\n",
      "step: 3500, value: 0.0016192147741094232, samples: 3, queries: 35\n",
      "step: 4000, value: 0.037779971957206726, samples: 3, queries: 38\n",
      "step: 4500, value: 0.0006777545204386115, samples: 3, queries: 41\n",
      "step: 5000, value: 0.0025243775453418493, samples: 3, queries: 44\n",
      "step: 5500, value: 0.0024407918099313974, samples: 3, queries: 48\n",
      "step: 6000, value: 0.00024308332649525255, samples: 4, queries: 52\n",
      "step: 6500, value: 3.808870678767562e-05, samples: 4, queries: 56\n",
      "step: 7000, value: 4.2629442759789526e-05, samples: 4, queries: 61\n",
      "step: 7500, value: 0.00011730636470019817, samples: 4, queries: 66\n",
      "step: 8000, value: 0.0001488456764491275, samples: 4, queries: 72\n",
      "step: 8500, value: 0.0007304971222765744, samples: 4, queries: 78\n",
      "step: 9000, value: 0.003314429195597768, samples: 4, queries: 85\n",
      "step: 9500, value: 0.00010718537669163197, samples: 4, queries: 92\n"
     ]
    }
   ],
   "source": [
    "dr.fit(x_train, y_train, ranking=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG@5: 0.1705 +- 0.261\n",
      "MAP: 0.1371 +- 0.1323\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.13706690295167515"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hlps.nDCGScorer_cls(dr, x_test, y_test, at=5)\n",
    "hlps.MAP_cls(dr, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
